{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The calculation of the invariant mass from the 4-vectors $p_1,p_2$ and what else is relevant can be learnt by a Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('dark_background')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use uproot https://github.com/scikit-hep/uproot \n",
    "# to read our root file.\n",
    "# Different to numpy_root we do not need a ROOT installation. Uproot knows the ROOT file format\n",
    "# uproot uses dictionaries to access the file content\n",
    "file = uproot.open(\"toy.root\")\n",
    "tree = file[\"tree\"]\n",
    "tree.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uproot provides an convenient interface to pandas\n",
    "df=tree.pandas.df([\"mass\",\"Pt1\", \"Pz1\", \"E1\",\"Phi1\",\"Eta1\",\"Pt2\", \"Pz2\",\"E2\",\"Phi2\",\"Eta2\",\"label\"])\n",
    "# reduced variables\n",
    "# df=tree.pandas.df([\"mass\",\"Pt1\", \"Pz1\",\"Eta1\",\"Pt2\", \"Pz2\",\"Eta2\",\"label\"])\n",
    "\n",
    "\n",
    "#select physical masses and subsample to make the example faster\n",
    "\n",
    "df=df[df.mass>0]\n",
    "# the file contains the signal first\n",
    "# we reshuffle and take a sub sample to make the example faster\n",
    "# left with about ~22k events\n",
    "df = df.sample(frac=0.25).reset_index(drop=True)\n",
    "df[df.label==1].hist([\"mass\"],bins=100,range=[0,3000])\n",
    "df[df.label==0].hist([\"mass\"],bins=100,range=[0,3000])\n",
    "print 'We have %d events' %len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we cut a mass window\n",
    "# and calculate the acc if we take all\n",
    "# events in the window as signal\n",
    "window  = (df.mass>645) & (df.mass<915)\n",
    "sigWin  = (df[window].label==1).sum()\n",
    "bgrdWin = (df[window].label==0).sum()\n",
    "allSig  = (df.label==1).sum()\n",
    "allBgrd = (df.label==0).sum()\n",
    "trueLabels=sigWin+(allBgrd-bgrdWin)\n",
    "allEvts  =allSig+allBgrd\n",
    "acc=trueLabels/float(allEvts)\n",
    "print acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# sklearn provides a convenient funtion to split our dataset randomly\n",
    "# into training and testing datasets \n",
    "# http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# we want to learn the label signal/background\n",
    "Y=df[\"label\"].values\n",
    "\n",
    "# and take Pt1,Eta1,Phi1,E1,Pt2,Eta2,Phi2,E2 as input features\n",
    "# using the calcuated mass would be to easy\n",
    "\n",
    "X=df.drop([\"mass\",\"label\"],axis=1).values\n",
    "\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "print 'X_train: ',type(X_train),X_train.shape,X_train.dtype\n",
    "print 'Y_train: ',type(Y_train),Y_train.shape,Y_train.dtype\n",
    "print 'X_test: ',type(X_test),X_test.shape,X_test.dtype\n",
    "print 'Y_test: ',type(Y_test),Y_test.shape,Y_test.dtype\n",
    "print 'We use', len(Y_train),'for training.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print df['Pt1'][0],X[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#move to classification\n",
    "# It is almost always a good idea to normalize the data before training\n",
    "# http://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py\n",
    "\n",
    "# Standardize features by removing the mean and scaling to unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# we keep the origina data untouched and create a scaled copy\n",
    "scaler = StandardScaler(copy=True) \n",
    "\n",
    "# we normalize our data\n",
    "X_scaled_train = scaler.fit_transform(X_train)\n",
    "# aplly the same transformation to the test sample\n",
    "X_scaled_test  = scaler.transform(X_test)\n",
    "print type(X_scaled_train ),type(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to show the scaling\n",
    "pt1        = X_train[:,0]\n",
    "pt1_scaled = X_scaled_train[:,0]\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.title('Pt1')\n",
    "plt.grid(True)\n",
    "#plt.yscale('log')\n",
    "plt.hist(pt1,bins=100)\n",
    "plt.subplot(122)\n",
    "plt.title('Pt1 - scaled')\n",
    "plt.grid(True)\n",
    "#plt.yscale('log')\n",
    "plt.hist(pt1_scaled,bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X_train[:,0].mean(),X_scaled_train[:,0].mean()\n",
    "print X_train[:,0].var(),X_scaled_train[:,0].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print Y_train.sum(),'signal events',(Y_train==0).sum(),'background events',len(Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We build our NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from time import time\n",
    "\n",
    "dtype  = torch.float\n",
    "#device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "\n",
    "# pyTorchViz is a little tool to visualize pyTorch's execution graph\n",
    "# https://github.com/szagoruyko/pytorchviz\n",
    "from torchviz import make_dot\n",
    "\n",
    "# to keep things simple\n",
    "# D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "\n",
    "D_in, H, D_out = X.shape[1], 10, 1\n",
    "print 'Number of features',D_in\n",
    "\n",
    "# Create input and output torch tensors from previous numpy array\n",
    "# with respect to these Tensors during the backward pass.\n",
    "x_train = torch.from_numpy(X_scaled_train).float().to(device)\n",
    "y_train = torch.from_numpy(Y_train).float().to(device)\n",
    "x_test  = torch.from_numpy(X_scaled_test).float().to(device)\n",
    "y_test  = torch.from_numpy(Y_test).float().to(device)\n",
    "\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "model.to(device)\n",
    "loss_fn = torch.nn.BCELoss(reduction='elementwise_mean')\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Tensors it should update.\n",
    "learning_rate = 0.008\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate,amsgrad=False)\n",
    "\n",
    "\n",
    "L=[]\n",
    "\n",
    "n_steps=50\n",
    "batch_size = 128\n",
    "\n",
    "import torch.utils.data\n",
    "dataset      = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "data_loader  = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle = True)\n",
    "n_inner=float(len(data_loader))\n",
    "\n",
    "t0=time()\n",
    "for epoch in range(n_steps):\n",
    "    av_loss=0\n",
    "    step=0\n",
    "    t = time()\n",
    "    for x,y in data_loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # Forward pass: compute predicted y by passing x to the model.\n",
    "        #y_pred = model(x).reshape(len(y))\n",
    "        y_pred = model(x).flatten()\n",
    "        # Compute and print loss.\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            av_loss+=loss.item()\n",
    "\n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable\n",
    "        # weights of the model). This is because by default, gradients are\n",
    "        # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "        # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model\n",
    "        # parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        #loss_train = loss_fn(model(x_train).flatten(),y_train).item()\n",
    "        av_loss/=n_inner # to gain some sensitivy to the inner loop\n",
    "        loss_test  = loss_fn(model(x_test).flatten(), y_test).item()\n",
    "    #torch.cuda.synchronize()\n",
    "    dt  = time()-t\n",
    "    tot = time()-t0\n",
    "    print \"Epoch %4d (%0.3f s tot: %0.3f s) av. loss train: %0.3f - lost test %0.3f\" \\\n",
    "    % (epoch, dt, tot, av_loss, loss_test)\n",
    "    L.append([av_loss,loss_test])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we plot the learning curve\n",
    "Xaxis = np.arange(0,len(L))\n",
    "plt.title('loss eveolution')\n",
    "lns=plt.plot(Xaxis,L)\n",
    "plt.legend(lns, ('train', 'test')) \n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('BCEloss/evt')\n",
    "#plt.yscale('log')\n",
    "plt.show()\n",
    "\n",
    "#visualize the network\n",
    "#out\n",
    "print model\n",
    "#Plot without loss: make_dot(model(x), params=dict(model.named_parameters()))\n",
    "#make_dot(loss,params=dict(model.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We note that the loss becomes flat and the test error is above the train error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply learned\n",
    "# get everything back to cpu\n",
    "model.cpu()\n",
    "\n",
    "x_test=x_test.cpu()\n",
    "y_test=y_test.cpu()\n",
    "\n",
    "y_pred = model(x_test).detach().numpy()\n",
    "y_true = y_test.numpy()\n",
    "\n",
    "plt.title('classifier')\n",
    "#plt.grid(False)\n",
    "plt.yscale('log')\n",
    "plt.hist(y_pred[y_true==1],bins=100,range=[0,1],label='signal')\n",
    "plt.hist(y_pred[y_true==0],bins=100,range=[0,1],label='backgrd')\n",
    "plt.legend()\n",
    "plt.xlabel('classifier output')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It seems that the separation works extremly good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the quality of the classifier can be checked by standard scores like\n",
    "# like accuracy and auc (area under the receiver operator curve, \n",
    "# i.e. true positive over false positive)\n",
    "# sklearn provides many useful tools for this\n",
    "\n",
    "from sklearn.metrics import accuracy_score,roc_curve,roc_auc_score\n",
    "acc = accuracy_score(y_true, y_pred>0.5, normalize=True)\n",
    "auc = roc_auc_score(y_true, y_pred>0.5)\n",
    "print acc,auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test on all\n",
    "X_scaled_all  = scaler.transform(X)\n",
    "Y_all = Y\n",
    "Y_pred = model(torch.tensor(X_scaled_all)).detach().numpy()\n",
    "\n",
    "#add columns to df\n",
    "df['y_pred']= Y_pred\n",
    "\n",
    "# we plot the invariant mass for the events that are selected as signal/background \n",
    "# by our NN classifier\n",
    "df[df.y_pred<0.5].mass.plot.hist(bins=100,range=[0,3000],label='pred. bgrd.')\n",
    "df[df.y_pred>0.5].mass.plot.hist(bins=100,range=[0,3000],label='pred. sig.')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What give the seperation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we replace one column after the other by its reshuffled version\n",
    "# with the help of a random index and evaluate the model \n",
    "# on the 1-bin-reshuffled data and print the accuracy\n",
    "\n",
    "# the variable names\n",
    "names=df.keys()[1:11]\n",
    "\n",
    "xall=torch.tensor(X_scaled_all)\n",
    "\n",
    "# an index to randomize\n",
    "idx=np.random.permutation(X_scaled_all.shape[0])\n",
    "\n",
    "# for our 10 parameters that runs from 0..9\n",
    "for i in range(X_scaled_all.shape[1]): \n",
    "    #replace one column by its permutation\n",
    "    xall[:,i]=xall[idx,i]\n",
    "    Y_pred = model(xall).detach().numpy()\n",
    "    acc = accuracy_score(Y_all, Y_pred>0.5, normalize=True)\n",
    "    print 'acc w/o %5s %0.4f'%(names[i],acc)\n",
    "    #reestablish the old order\n",
    "    xall[:,i]=torch.tensor(X_scaled_all[:,i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try to reduce the number of variables and see how big the influence on the accuracy is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch04]",
   "language": "python",
   "name": "conda-env-torch04-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
